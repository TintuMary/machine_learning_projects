{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Encoder_CNN_mnist_AutoEncoder_&_NoiseReduction.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNYxE/JTDPCcWzuqKMt0LLg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"tcWyL3Z-Fzb_","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","#import matplotlib\n","\n","\n","import matplotlib.pylab as plt\n","\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.datasets import mnist\n","\n","import tensorflow_hub as hub\n","import numpy as np\n","import pandas as pd\n","import argparse\n","import cv2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjiHXW9eRUP7","colab_type":"code","outputId":"5d7e618b-6bf2-455b-ccf0-78049bc4b15e","executionInfo":{"status":"ok","timestamp":1586205527062,"user_tz":300,"elapsed":6979,"user":{"displayName":"abhishek khandelwal","photoUrl":"https://lh6.googleusercontent.com/-9_JeiTM6h3w/AAAAAAAAAAI/AAAAAAAAKT0/50pUZ0hooPQ/s64/photo.jpg","userId":"12088551721026822394"}},"colab":{"base_uri":"https://localhost:8080/","height":360}},"source":["!pip install q keras==2.3.0\n","import keras\n","keras.__version__"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting q\n","  Downloading https://files.pythonhosted.org/packages/53/bc/51619d89e0bd855567e7652fa16d06f1ed36a85f108a7fe71f6629bf719d/q-2.6-py2.py3-none-any.whl\n","Collecting keras==2.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/18/2e1ef121e5560ac24c7ac9e363aa5fa7006c40563c989e7211aba95b793a/Keras-2.3.0-py2.py3-none-any.whl (377kB)\n","\u001b[K     |████████████████████████████████| 378kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.12.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (2.10.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.18.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.0.8)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.4.1)\n","Installing collected packages: q, keras\n","  Found existing installation: Keras 2.2.5\n","    Uninstalling Keras-2.2.5:\n","      Successfully uninstalled Keras-2.2.5\n","Successfully installed keras-2.3.0 q-2.6\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["'2.3.0'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"IUZgMC0_RbZq","colab_type":"code","colab":{}},"source":["((trainX, _), (testX, _)) = mnist.load_data()\n","trainX = np.expand_dims(trainX, axis=-1)\n","testX = np.expand_dims(testX, axis=-1)\n","trainX = trainX.astype(\"float32\") / 255.0\n","testX = testX.astype(\"float32\") / 255.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x9ZjAwh0OnDu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"cb50678c-16bc-4fd3-eb22-3a9e6aff004b","executionInfo":{"status":"ok","timestamp":1586205758431,"user_tz":300,"elapsed":754,"user":{"displayName":"abhishek khandelwal","photoUrl":"https://lh6.googleusercontent.com/-9_JeiTM6h3w/AAAAAAAAAAI/AAAAAAAAKT0/50pUZ0hooPQ/s64/photo.jpg","userId":"12088551721026822394"}}},"source":["X = trainX[0].reshape([28, 28]);\n","plt.gray()\n","plt.imshow(X)\n","#Actually displaying the plot if you are not in interactive mode\n","plt.show()"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEARhB4Ig7EAQ/w8ie3GmjcGk5QAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"OAqgA_WLR_4L","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import Conv2DTranspose\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Reshape\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import backend as K\n","import numpy as np\n","\n","class ConvAutoencoder:\n","\t@staticmethod\n","\tdef build(width, height, depth, filters=(32, 64), latentDim=16):\n","\t\t# initialize the input shape to be \"channels last\" along with\n","\t\t# the channels dimension itself\n","\t\t# channels dimension itself\n","\t\tinputShape = (height, width, depth)\n","\t\tchanDim = -1\n","\n","\t\t# define the input to the encoder\n","\t\tinputs = Input(shape=inputShape)\n","\t\tx = inputs\n","\n","\t\t# loop over the number of filters\n","\t\tfor f in filters:\n","\t\t\t# apply a CONV => RELU => BN operation\n","\t\t\tx = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n","\t\t\tx = LeakyReLU(alpha=0.2)(x)\n","\t\t\tx = BatchNormalization(axis=chanDim)(x)\n","\n","\t\t# flatten the network and then construct our latent vector\n","\t\tvolumeSize = K.int_shape(x)\n","\t\tx = Flatten()(x)\n","\t\tlatent = Dense(latentDim)(x)\n","\n","\t\t# build the encoder model\n","\t\tencoder = Model(inputs, latent, name=\"encoder\")\n","\n","\t\t# start building the decoder model which will accept the\n","\t\t# output of the encoder as its inputs\n","\t\tlatentInputs = Input(shape=(latentDim,))\n","\t\tx = Dense(np.prod(volumeSize[1:]))(latentInputs)\n","\t\tx = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n","\n","\t\t# loop over our number of filters again, but this time in\n","\t\t# reverse order\n","\t\tfor f in filters[::-1]:\n","\t\t\t# apply a CONV_TRANSPOSE => RELU => BN operation\n","\t\t\tx = Conv2DTranspose(f, (3, 3), strides=2,\n","\t\t\t\tpadding=\"same\")(x)\n","\t\t\tx = LeakyReLU(alpha=0.2)(x)\n","\t\t\tx = BatchNormalization(axis=chanDim)(x)\n","\n","\t\t# apply a single CONV_TRANSPOSE layer used to recover the\n","\t\t# original depth of the image\n","\t\tx = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n","\t\toutputs = Activation(\"sigmoid\")(x)\n","\n","\t\t# build the decoder model\n","\t\tdecoder = Model(latentInputs, outputs, name=\"decoder\")\n","\n","\t\t# our autoencoder is the encoder + decoder\n","\t\tautoencoder = Model(inputs, decoder(encoder(inputs)),\n","\t\t\tname=\"autoencoder\")\n","\n","\t\t# return a 3-tuple of the encoder, decoder, and autoencoder\n","\t\treturn (encoder, decoder, autoencoder)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HdFUvp-KSGCj","colab_type":"code","outputId":"5e49cfdc-39dd-41d5-a599-b80de57f302d","executionInfo":{"status":"ok","timestamp":1586189879553,"user_tz":300,"elapsed":933,"user":{"displayName":"abhishek khandelwal","photoUrl":"https://lh6.googleusercontent.com/-9_JeiTM6h3w/AAAAAAAAAAI/AAAAAAAAKT0/50pUZ0hooPQ/s64/photo.jpg","userId":"12088551721026822394"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(\"[INFO] building autoencoder...\")\n","(encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1)\n","opt = Adam(lr=1e-3)\n","autoencoder.compile(loss=\"mse\", optimizer=opt)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[INFO] building autoencoder...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B107pGYzSNI8","colab_type":"code","outputId":"bfd55736-1d60-436c-c0a2-8edf8f6e6acc","executionInfo":{"status":"ok","timestamp":1586190085917,"user_tz":300,"elapsed":136658,"user":{"displayName":"abhishek khandelwal","photoUrl":"https://lh6.googleusercontent.com/-9_JeiTM6h3w/AAAAAAAAAAI/AAAAAAAAKT0/50pUZ0hooPQ/s64/photo.jpg","userId":"12088551721026822394"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["EPOCHS = 1\n","BS = 32\n","\n","H = autoencoder.fit(\n","\ttrainX, trainX,\n","\tvalidation_data=(testX, testX),\n","\tepochs=EPOCHS,\n","\tbatch_size=BS)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1875/1875 [==============================] - 135s 72ms/step - loss: 0.0194 - val_loss: 0.0106\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sbKAlzTcSa4b","colab_type":"code","colab":{}},"source":["N = np.arange(0, EPOCHS)\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n","plt.title(\"Training Loss and Accuracy\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend(loc=\"lower left\")\n","plt.savefig(\"plot.png\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PC9_V3oVio0","colab_type":"code","outputId":"4b91b19e-581f-47c3-b067-cee24ca8a1f5","executionInfo":{"status":"ok","timestamp":1586190914612,"user_tz":300,"elapsed":7185,"user":{"displayName":"abhishek khandelwal","photoUrl":"https://lh6.googleusercontent.com/-9_JeiTM6h3w/AAAAAAAAAAI/AAAAAAAAKT0/50pUZ0hooPQ/s64/photo.jpg","userId":"12088551721026822394"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["decoded = autoencoder.predict(testX)\n","outputs = None\n","#samples = [1 31 141 234]\n","# loop over our number of output samples\n","for i in range(0, 10):\n","\t# grab the original image and reconstructed image\n","\toriginal = (testX[i] * 255).astype(\"uint8\")\n","\trecon = (decoded[i] * 255).astype(\"uint8\")\n","\n","\t# stack the original and reconstructed image side-by-side\n","\toutput = np.hstack([original, recon])\n","\n","\t# if the outputs array is empty, initialize it as the current\n","\t# side-by-side image display\n","\tif outputs is None:\n","\t\toutputs = output\n","\n","\t# otherwise, vertically stack the outputs\n","\telse:\n","\t\toutputs = np.vstack([outputs, output])\n","  \n","cv2.imwrite(\"output.png\", outputs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["ERROR! Session/line number was not unique in database. History logging moved to new session 60\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"1Ubq7T4mO3FW","colab_type":"text"},"source":["So far, we built a model for Auto Encoder using CNN. Below section is to -\n","a. Add a noise in the input image\n","b. train a CNN auto encoder model on top of these train images\n","c. Aim is to remove these noise from input image and predict noise free image"]},{"cell_type":"code","metadata":{"id":"dlDtW_cIAcgu","colab_type":"code","colab":{}},"source":["# sample noise from a random normal distribution centered at 0.5 (since\n","# our images lie in the range [0, 1]) and a standard deviation of 0.5\n","trainNoise = np.random.normal(loc=0.5, scale=0.5, size=trainX.shape)\n","testNoise = np.random.normal(loc=0.5, scale=0.5, size=testX.shape)\n","trainXNoisy = np.clip(trainX + trainNoise, 0, 1)\n","testXNoisy = np.clip(testX + testNoise, 0, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ld8hIsx-CMAV","colab_type":"code","outputId":"2318d3e8-5c72-4457-b481-c62875185652","executionInfo":{"status":"ok","timestamp":1586202463968,"user_tz":300,"elapsed":1552,"user":{"displayName":"abhishek khandelwal","photoUrl":"https://lh6.googleusercontent.com/-9_JeiTM6h3w/AAAAAAAAAAI/AAAAAAAAKT0/50pUZ0hooPQ/s64/photo.jpg","userId":"12088551721026822394"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(\"[INFO] building autoencoder...\")\n","(encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1)\n","opt = Adam(lr=1e-3)\n","autoencoder.compile(loss=\"mse\", optimizer=opt)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[INFO] building autoencoder...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XCapxJKUCNXZ","colab_type":"code","outputId":"c2f5638d-febd-4726-cc54-bfcf75740d70","executionInfo":{"status":"ok","timestamp":1586202629445,"user_tz":300,"elapsed":141923,"user":{"displayName":"abhishek khandelwal","photoUrl":"https://lh6.googleusercontent.com/-9_JeiTM6h3w/AAAAAAAAAAI/AAAAAAAAKT0/50pUZ0hooPQ/s64/photo.jpg","userId":"12088551721026822394"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["EPOCHS = 1\n","BS = 32\n","H = autoencoder.fit(\n","\ttrainXNoisy, trainX,\n","\tvalidation_data=(testXNoisy, testX),\n","\tepochs=EPOCHS,\n","\tbatch_size=BS)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1875/1875 [==============================] - 140s 75ms/step - loss: 0.0279 - val_loss: 0.0193\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ve04cqrtCVQE","colab_type":"code","colab":{}},"source":["N = np.arange(0, EPOCHS)\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n","plt.title(\"Training Loss and Accuracy\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend(loc=\"lower left\")\n","plt.savefig(\"plot1.png\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0X2m88oDCY-d","colab_type":"code","outputId":"551680c9-608e-4f83-b3eb-14040ae54eb3","executionInfo":{"status":"ok","timestamp":1586202754011,"user_tz":300,"elapsed":7577,"user":{"displayName":"abhishek khandelwal","photoUrl":"https://lh6.googleusercontent.com/-9_JeiTM6h3w/AAAAAAAAAAI/AAAAAAAAKT0/50pUZ0hooPQ/s64/photo.jpg","userId":"12088551721026822394"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# use the convolutional autoencoder to make predictions on the\n","# testing images, then initialize our list of output images\n","print(\"[INFO] making predictions...\")\n","decoded = autoencoder.predict(testXNoisy)\n","outputs = None\n","\n","# loop over our number of output samples\n","for i in range(0, 10):\n","\t# grab the original image and reconstructed image\n","\toriginal = (testXNoisy[i] * 255).astype(\"uint8\")\n","\trecon = (decoded[i] * 255).astype(\"uint8\")\n","\n","\t# stack the original and reconstructed image side-by-side\n","\toutput = np.hstack([original, recon])\n","\n","\t# if the outputs array is empty, initialize it as the current\n","\t# side-by-side image display\n","\tif outputs is None:\n","\t\toutputs = output\n","\n","\t# otherwise, vertically stack the outputs\n","\telse:\n","\t\toutputs = np.vstack([outputs, output])\n","\n","# save the outputs image to disk\n","cv2.imwrite(\"output1.png\", outputs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[INFO] making predictions...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":10}]}]}