{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part_6-G_Audio_Classification_VGG16_GD_Image_With_FineTuning.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNaMSDkb0kiEfEv+HQdbz2r"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"KIPr0ZLo2Int","colab_type":"code","colab":{}},"source":["!pip install q keras==2.3.0\n","!pip install tensorflow_hub"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJY1fZct2rnJ","colab_type":"code","outputId":"79a8f386-0a8f-403a-90fa-93fc69d7e001","executionInfo":{"status":"ok","timestamp":1588111903544,"user_tz":300,"elapsed":3436,"user":{"displayName":"Abhishek Khandelwal","photoUrl":"","userId":"01269108848039127440"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import matplotlib.pylab as plt\n","\n","%tensorflow_version 2.1.0\n","import tensorflow as tf\n","\n","import tensorflow_hub as hub\n","\n","from keras.models import Sequential\n","from keras.layers.core import Dense\n","from keras.optimizers import SGD\n","from keras.utils import to_categorical\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.applications import VGG16\n","from keras.layers.core import Dropout\n","from keras.layers.core import Flatten\n","from keras.layers import Input\n","from keras.models import Model\n","\n","from sklearn.metrics import classification_report\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.preprocessing import LabelEncoder\n","from keras.utils import np_utils\n","import pickle\n","import cv2\n","import os\n","import h5py\n","import keras\n","print(keras.__version__)\n","print(tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `2.1.0`. This will be interpreted as: `2.x`.\n","\n","\n","TensorFlow 2.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["2.3.0\n","2.2.0-rc3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NdMeOTN025V_","colab_type":"code","outputId":"59694bc3-fdd2-4c38-977d-437c8a256d12","executionInfo":{"status":"ok","timestamp":1588111933583,"user_tz":300,"elapsed":25991,"user":{"displayName":"Abhishek Khandelwal","photoUrl":"","userId":"01269108848039127440"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8Uf5uQ3D4HwB","colab_type":"code","colab":{}},"source":["def plot_training(H, N, plotPath):\n","\t# construct a plot that plots and saves the training history\n","\tplt.style.use(\"ggplot\")\n","\tplt.figure()\n","\tplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n","\tplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n","\tplt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n","\tplt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n","\tplt.title(\"Training Loss and Accuracy\")\n","\tplt.xlabel(\"Epoch #\")\n","\tplt.ylabel(\"Loss/Accuracy\")\n","\tplt.legend(loc=\"lower left\")\n","\tplt.savefig(plotPath)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgzH5u9fqJdj","colab_type":"code","outputId":"12bd9a92-d311-4d0e-9030-2f28c6ee2d27","executionInfo":{"status":"ok","timestamp":1588111986155,"user_tz":300,"elapsed":33891,"user":{"displayName":"Abhishek Khandelwal","photoUrl":"","userId":"01269108848039127440"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["data_root='/content/drive/My Drive/Audio_Quality_Analysis/data/dataset_1/train'\n","IMAGE_SHAPE = (224, 224)\n","TRAINING_DATA_DIR = str(data_root)\n","\n","datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n","\n","valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\n","valid_generator = valid_datagen.flow_from_directory( TRAINING_DATA_DIR, subset=\"validation\",\n","                                                      shuffle=True, target_size=IMAGE_SHAPE)\n","\n","train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\n","train_generator = train_datagen.flow_from_directory(TRAINING_DATA_DIR, subset=\"training\",\n","                                                      shuffle=True, target_size=IMAGE_SHAPE)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 2240 images belonging to 2 classes.\n","Found 8963 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UOlqPRzXqWGj","colab_type":"code","outputId":"369ca102-33ec-4d45-8a4e-25413074a285","executionInfo":{"status":"ok","timestamp":1588112867564,"user_tz":300,"elapsed":843,"user":{"displayName":"Abhishek Khandelwal","photoUrl":"","userId":"01269108848039127440"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["dataset_labels = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\n","print(dataset_labels)\n","dataset_labels = np.array([key.title() for key, value in dataset_labels])\n","print(dataset_labels)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('good', 0), ('noise', 1)]\n","['Good' 'Noise']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n4nrmG9OqR7E","colab_type":"code","outputId":"43556f5a-e4b6-49c0-e12a-a8c21e7034e2","executionInfo":{"status":"ok","timestamp":1588000634267,"user_tz":300,"elapsed":810,"user":{"displayName":"Abhishek Khandelwal","photoUrl":"","userId":"01269108848039127440"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["le = LabelEncoder()\n","labels = le.fit_transform(dataset_labels)\n","#labels = np_utils.to_categorical(labels, 3)\n","print(labels)\n","\n","with open('/content/drive/My Drive/Audio_Quality_Analysis/model/labelEncoderFile_VGG16_TL_FineTuning_04262020_v1.pkl', 'wb') as fid:\n","    pickle.dump(le, fid)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zod9iNhE3FPQ","colab_type":"code","outputId":"751182f6-cdfe-471b-dfee-4ef778718937","executionInfo":{"status":"ok","timestamp":1588112877996,"user_tz":300,"elapsed":2485,"user":{"displayName":"Abhishek Khandelwal","photoUrl":"","userId":"01269108848039127440"}},"colab":{"base_uri":"https://localhost:8080/","height":986}},"source":["baseModel = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n","\n","# construct the head of the model that will be placed on top of the\n","# the base model\n","headModel = baseModel.output\n","headModel = Flatten(name=\"flatten\")(headModel)\n","headModel = Dense(512, activation=\"relu\")(headModel)\n","headModel = Dropout(0.5)(headModel)\n","headModel = Dense(2, activation=\"softmax\")(headModel)\n","\n","model = Model(inputs=baseModel.input, outputs=headModel)\n","\n","for layer in baseModel.layers:\n","\tlayer.trainable = False\n","\n","model.summary()\n","\n","print(\"[INFO] compiling model...\")\n","opt = SGD(lr=1e-4, momentum=0.9)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               12845568  \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 2)                 1026      \n","=================================================================\n","Total params: 27,561,282\n","Trainable params: 12,846,594\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n","[INFO] compiling model...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f5jX1r6QqsAM","colab_type":"code","colab":{}},"source":["steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\n","val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\n","\n","hist = model.fit_generator( train_generator, epochs=14, verbose=1,\n","                            steps_per_epoch=steps_per_epoch,\n","                            validation_data=valid_generator,\n","                            validation_steps=val_steps_per_epoch\n","                           ).history\n","\n","model.save(\"/content/drive/My Drive/Audio_Quality_Analysis/model/AudioQuality_VGG16_TL_GD_FineTuning_V1_04227.h5\")\n","model.save_weights(\"/content/drive/My Drive/Audio_Quality_Analysis/model/AudioQualityWeights_VGG16_TL_GD_FineTuning_V1_0427.h5\")\n","\n","plot_training(hist, 20, 'warmup.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DSErwelQ5E4_","colab_type":"code","colab":{}},"source":["for layer in baseModel.layers[15:]:\n","\tlayer.trainable = True\n","\n","# loop over the layers in the model and show which ones are trainable\n","# or not\n","for layer in baseModel.layers:\n","\tprint(\"{}: {}\".format(layer, layer.trainable))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uB79PU3s5IsU","colab_type":"code","colab":{}},"source":["print(\"[INFO] re-compiling model...\")\n","opt = SGD(lr=1e-4, momentum=0.9)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n","\tmetrics=[\"accuracy\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWiGRzPq5JhM","colab_type":"code","colab":{}},"source":["steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\n","val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\n","\n","hist = model.fit_generator( train_generator, epochs=10, verbose=1,\n","                            steps_per_epoch=steps_per_epoch,\n","                            validation_data=valid_generator,\n","                            validation_steps=val_steps_per_epoch\n","                           ).history\n","\n","model.save(\"/content/drive/My Drive/Audio_Quality_Analysis/model/AudioQuality_VGG16_TL_GD_FineTuning_V1_04228.h5\")\n","model.save_weights(\"/content/drive/My Drive/Audio_Quality_Analysis/model/AudioQualityWeights_VGG16_TL_GD_FineTuning_V1_0428.h5\")\n","\n","plot_training(hist, 10, 'warmup.png')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4F3RXXcKRzr","colab_type":"code","colab":{}},"source":["from keras import models\n","model = tf.keras.models.load_model('/content/drive/My Drive/Audio_Quality_Analysis/model/AudioQuality_VGG16_TL_GD_FineTuning_V1_04227.h5')\n","model.load_weights('/content/drive/My Drive/Audio_Quality_Analysis/model/AudioQualityWeights_VGG16_TL_GD_FineTuning_V1_0427.h5')\n","model.summary()"],"execution_count":0,"outputs":[]}]}