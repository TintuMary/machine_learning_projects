{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this notebook, providing information about how to use Librosa and Pydub library for.\n",
    "A. Using Librosa (librosa.effects.split), Get the non-silent intervals in given audio file\n",
    "B. Using Pydub (split_on_silence), split the audio file into multiple chunks with non-silent data. Means splitted data wont contain any silent data\n",
    "C. Using Pydub (detect_nonsilent), get the non-silent intervals in given audio file \n",
    "D. Using Pydub (detect_silence), get the silent intervals in given audio file \n",
    "E. Using FFMPEG, get the non-silent intervals in given audio file\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import pysrt\n",
    "import time\n",
    "import subprocess\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "import librosa\n",
    "import librosa.display as display\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import  detect_nonsilent\n",
    "from pydub.silence import split_on_silence\n",
    "from pydub.silence import detect_silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_silence_using_ffmpeg():\n",
    "    '''\n",
    "    Command line command to detect silence range using ffmpeg\n",
    "    ffmpeg -i \"input.mov\" -af silencedetect=noise=-30dB:d=0.5 -f null - 2> vol.txt\n",
    "    '''\n",
    "    input_video_file = 'D:/Abhishek/Machine Learning Models/Subtitle-Sync/github-repo/SubtitleSynchronizer/data/THENUN.mp4'\n",
    "\n",
    "    extract_sound_cmd = [\n",
    "         'ffmpeg',\n",
    "         '-i', input_video_file,\n",
    "         '-af', 'silencedetect=noise=-30dB:d=0.5',\n",
    "         '-f', 'null', '-', '2>',\n",
    "         'vol.txt'\n",
    "     ]\n",
    "    output = subprocess.call(extract_sound_cmd)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "input_audio_file = 'D:/Abhishek/Machine Learning Models/Subtitle-Sync/video/New/THENUN.flac'\n",
    "samples, sr = librosa.load(input_audio_file)\n",
    "print('duration: ', len(samples)/sr)\n",
    "\n",
    "non_silent_interval = librosa.effects.split(samples)  #top_db -> this parameter can also be specified.\n",
    "\n",
    "for silence_range in non_silent_interval:\n",
    "    start_second = silence_range[0]/sr\n",
    "    end_second = silence_range[1]/sr\n",
    "    \n",
    "    start = str(datetime.timedelta(seconds=start_second))\n",
    "    end = str(datetime.timedelta(seconds=end_second))\n",
    "    \n",
    "    print(start, ' : ', end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_width= 2\n",
      "channel_count= 2\n",
      "duration_in_sec= 600.0\n",
      "frame_rate= 44100\n",
      "lenght : 600000\n"
     ]
    }
   ],
   "source": [
    "filename = 'D:/Abhishek/Machine Learning Models/Subtitle-Sync/video/New/output/THENUN/vocals.wav'\n",
    "audio_signal = AudioSegment.from_file(filename, \"wav\")\n",
    "\n",
    "channel_count = audio_signal.channels    #Get channels\n",
    "sample_width = audio_signal.sample_width #Get sample width\n",
    "duration_in_sec = len(audio_signal) / 1000#Length of audio in sec\n",
    "sample_rate = audio_signal.frame_rate\n",
    "\n",
    "print (\"sample_width=\", sample_width )\n",
    "print (\"channel_count=\", channel_count)\n",
    "print (\"duration_in_sec=\", duration_in_sec) \n",
    "print (\"frame_rate=\", sample_rate)\n",
    "print('lenght :', len(audio_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_on_silence(audio_signal,min_silence_len=280,silence_thresh=-33,keep_silence=150)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(i, ' : ', len(chunk))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_signal = chunks[0]\n",
    "channel_count = audio_signal.channels    #Get channels\n",
    "sample_width = audio_signal.sample_width #Get sample width\n",
    "duration_in_sec = len(audio_signal) / 1000#Length of audio in sec\n",
    "sample_rate = audio_signal.frame_rate\n",
    "\n",
    "print (\"sample_width=\", sample_width )\n",
    "print (\"channel_count=\", channel_count)\n",
    "print (\"duration_in_sec=\", duration_in_sec) \n",
    "print (\"frame_rate=\", sample_rate)\n",
    "print('lenght :', len(audio_signal))\n",
    "\n",
    "audio_signal.export(\"pydub_mute_wave.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound = AudioSegment.from_file(\"D:/Abhishek/Machine Learning Models/Subtitle-Sync/vocal_mute_signals/vocals/video000/vocals.wav\", format=\"mp3\")\n",
    "\n",
    "silent_ranges = detect_silence(sound, min_silence_len=1000, silence_thresh=-33)\n",
    "for silence_range in silent_ranges:\n",
    "    start = str(datetime.timedelta(seconds=silence_range[0]/1000))\n",
    "    end = str(datetime.timedelta(seconds=silence_range[1]/1000))\n",
    "    print(start, ' : ', end)\n",
    "    \n",
    "    \n",
    "nonsilent_audio_rnage = detect_nonsilent(sound,min_silence_len=1000,silence_thresh=-33)\n",
    "for silence_range in nonsilent_audio_rnage:\n",
    "    start = str(datetime.timedelta(seconds=silence_range[0]/1000))\n",
    "    end = str(datetime.timedelta(seconds=silence_range[1]/1000))\n",
    "    print(start, ' : ', end)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorFlowEnv",
   "language": "python",
   "name": "tensorflowenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
