{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this notebook, different techniques have been covered for data transformation which can be applied during data pre-processing phase\n",
    "\n",
    "A. Simple Imputer\n",
    "B. Label Encoder\n",
    "C. One Hot Encoder\n",
    "D. Standard Scalar\n",
    "E. How to save these transformation logic into pickle file\n",
    "F. How to use these pickle files on Test/Validation data while validating/testing model\n",
    "G. At the end, how to configure all these data preprocessing techniques into pipeline.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Country   Age   Salary Purchased\n",
      "0    France  44.0  72000.0        No\n",
      "1     Spain  27.0  48000.0       Yes\n",
      "2   Germany  34.0  56000.0        No\n",
      "3     India  24.0  61000.0       Yes\n",
      "4     Spain  38.0  61000.0        No\n",
      "5   Germany  40.0      NaN       Yes\n",
      "6    France  35.0  58000.0       Yes\n",
      "7     India   NaN  52000.0        No\n",
      "8    France  48.0  79000.0       Yes\n",
      "9   Germany  50.0  83000.0        No\n",
      "10   France  37.0  67000.0       Yes\n",
      "11    India  44.0      0.0       Yes\n",
      "This prints the column # 2 i.e. 'Age'. Range starts from '0' here.\n",
      "[44.0 27.0 34.0 24.0 38.0 40.0 35.0 nan 48.0 50.0 37.0 44.0]\n",
      "This prints the column #3 and #2 i.e. 'Age' and 'Salary'. Range starts from '0' here.\n",
      "[[44.0 72000.0]\n",
      " [27.0 48000.0]\n",
      " [34.0 56000.0]\n",
      " [24.0 61000.0]\n",
      " [38.0 61000.0]\n",
      " [40.0 nan]\n",
      " [35.0 58000.0]\n",
      " [nan 52000.0]\n",
      " [48.0 79000.0]\n",
      " [50.0 83000.0]\n",
      " [37.0 67000.0]\n",
      " [44.0 0.0]]\n",
      "This prints the column #3 and #2 values only for Row # '2' and '3'.\n",
      "[[27.0 48000.0]\n",
      " [34.0 56000.0]]\n",
      "This prints the row # 1. Range starts from '0' here.\n",
      "  Country   Age   Salary Purchased\n",
      "0  France  44.0  72000.0        No\n",
      "This prints the rows indexing from '4' to '8'. Range starts from '0' here.\n",
      "   Country   Age   Salary Purchased\n",
      "4    Spain  38.0  61000.0        No\n",
      "5  Germany  40.0      NaN       Yes\n",
      "6   France  35.0  58000.0       Yes\n",
      "7    India   NaN  52000.0        No\n",
      "8   France  48.0  79000.0       Yes\n",
      "This prints the column #3 and #2 i.e. 'Age' and 'Salary'. Range starts from '0' here.\n",
      "     Age   Salary\n",
      "0   44.0  72000.0\n",
      "1   27.0  48000.0\n",
      "2   34.0  56000.0\n",
      "3   24.0  61000.0\n",
      "4   38.0  61000.0\n",
      "5   40.0      NaN\n",
      "6   35.0  58000.0\n",
      "7    NaN  52000.0\n",
      "8   48.0  79000.0\n",
      "9   50.0  83000.0\n",
      "10  37.0  67000.0\n",
      "11  44.0      0.0\n",
      "This prints the last column i.e. 'Purchased'\n",
      "0      No\n",
      "1     Yes\n",
      "2      No\n",
      "3     Yes\n",
      "4      No\n",
      "5     Yes\n",
      "6     Yes\n",
      "7      No\n",
      "8     Yes\n",
      "9      No\n",
      "10    Yes\n",
      "11    Yes\n",
      "Name: Purchased, dtype: object\n",
      "This prints the last column value of last row.\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "data = [['France',44.0,72000.0,'No'], ['Spain',27.0,48000.0,'Yes'], ['Germany',34.0,56000.0,'No'],\n",
    "       ['India',24.0,61000.0,'Yes'], ['Spain',38,61000,'No'], ['Germany',40,'?','Yes'], ['France',35,58000,'Yes'],\n",
    "       ['India','?',52000,'No'], ['France',48,79000,'Yes'], ['Germany',50,83000,'No'], ['France',37,67000,'Yes'], \n",
    "       ['India',44,0,'Yes']]\n",
    "\n",
    "dataset = pd.DataFrame(data, columns=['Country', 'Age', 'Salary', 'Purchased'])\n",
    "dataset = dataset.replace('?', np.NaN)\n",
    "# or \n",
    "#dataset=dataset.applymap(lambda x: np.nan if x == '?' else x)\n",
    "\n",
    "X = dataset.iloc[:,:-1].values\n",
    "Y = dataset.iloc[:,3].values\n",
    "\n",
    "print(dataset.head(12))\n",
    "\n",
    "print(\"This prints the column # 2 i.e. 'Age'. Range starts from '0' here.\")\n",
    "print(X[:,1])\n",
    "\n",
    "print(\"This prints the column #3 and #2 i.e. 'Age' and 'Salary'. Range starts from '0' here.\")\n",
    "print(X[:,1:3])\n",
    "\n",
    "print(\"This prints the column #3 and #2 values only for Row # '2' and '3'.\")\n",
    "print(X[1:3,1:3])\n",
    "\n",
    "print(\"This prints the row # 1. Range starts from '0' here.\")\n",
    "print(dataset[:1])\n",
    "\n",
    "print(\"This prints the rows indexing from '4' to '8'. Range starts from '0' here.\")\n",
    "print(dataset[4:9])\n",
    "\n",
    "print(\"This prints the column #3 and #2 i.e. 'Age' and 'Salary'. Range starts from '0' here.\")\n",
    "print(dataset.iloc[:,1:3])\n",
    "\n",
    "print(\"This prints the last column i.e. 'Purchased'\")\n",
    "print(dataset.iloc[:,-1])\n",
    "\n",
    "print(\"This prints the last column value of last row.\")\n",
    "print(dataset.iloc[-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                                    *** How to handle missing data [SimpleImputer] ****\n",
    "A. There are different ways to handel missig data in dataset depends on whether column is continious or categorical such as\n",
    "    a.  remove rows which are having missing data - here we can set some threshold as well like if a row contains missing value \n",
    "        in more than 3 columns then only drop that row\n",
    "        for e.g.  X = X.dropna(subset = ['Age', 'Salary'])  or simply - X= X.dropna()\n",
    "    \n",
    "    b. remove columns which are having missing value - again we can configure some threshold here like if a column contains 20%\n",
    "        or more missing values then drop that column\n",
    "        for e.g.  X.drop(['Age', 'Salary'], axis = 1, inplace = True)\n",
    "    \n",
    "    c. or replace empty values with some logical values \n",
    "        1. In case of numeric values- we can replace empty values with 'mean'/'median' values of that column with fillna \n",
    "        method or we can use SimpleImputer api too\n",
    "            X['Age'].fillna(X['Age'].mean, inplace=true)\n",
    "        \n",
    "        2. In case of categorical attribute, consider empty value as a unique value and treat them accordingly.\n",
    "        \n",
    "B. SimpleImputer is used to handle missing data for continious attributes i.e. Age and Salary\n",
    "C. It needs to be saved as a file so that it can be used on Test data\n",
    "\n",
    "'''\n",
    "\n",
    "imputer = SimpleImputer(missing_values =np.nan,strategy = 'mean')\n",
    "imputer = imputer.fit(X[:,1:3])\n",
    "X[:,1:3] = imputer.transform(X[:,1:3])\n",
    "\n",
    "pickle_label_out = open(\"SimpleImputer_dataprocessing.pickle\",\"wb\")\n",
    "pickle.dump(imputer, pickle_label_out)\n",
    "pickle_label_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Label Encoder\n",
      "[[0 44.0 72000.0]\n",
      " [3 27.0 48000.0]\n",
      " [1 34.0 56000.0]\n",
      " [2 24.0 61000.0]\n",
      " [3 38.0 61000.0]\n",
      " [1 40.0 57909.09090909091]\n",
      " [0 35.0 58000.0]\n",
      " [2 38.27272727272727 52000.0]\n",
      " [0 48.0 79000.0]\n",
      " [1 50.0 83000.0]\n",
      " [0 37.0 67000.0]\n",
      " [2 44.0 0.0]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "                                    *** ONE HOT ENCODING ****\n",
    "A. Transform categorical data into Continious data via HotEncoder (or Label Encoder)\n",
    "B. In case of Label encoder- For given categorical feature, all unique values are getting converted into unique \n",
    "   numeric correspoding value\n",
    "C. As there is only one categorical attribute i.e. 'Country' at index '0' so we are applying encoding only for index '0' element\n",
    "D. SAVE these encoders as a file so that it can be used further on test/validation data while validation/testing a model\n",
    "\n",
    "'''                                    \n",
    " \n",
    "labelencoder = LabelEncoder()\n",
    "X[:, 0] = labelencoder.fit_transform(X[:, 0])\n",
    "print('After Label Encoder')\n",
    "print(X)\n",
    "\n",
    "labelencoder_Y=LabelEncoder()\n",
    "Y=labelencoder_Y.fit_transform(Y)\n",
    "\n",
    "\n",
    "pickle_label_out = open(\"label_encoded_dataprocessing_1.pickle\",\"wb\")\n",
    "pickle.dump(labelencoder, pickle_label_out)\n",
    "pickle_label_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After One Hot Encoder\n",
      "[[1.0, 0.0, 0.0, 0.0, 44.0, 72000.0], [0.0, 0.0, 0.0, 1.0, 27.0, 48000.0], [0.0, 1.0, 0.0, 0.0, 34.0, 56000.0], [0.0, 0.0, 1.0, 0.0, 24.0, 61000.0], [0.0, 0.0, 0.0, 1.0, 38.0, 61000.0], [0.0, 1.0, 0.0, 0.0, 40.0, 57909.09090909091], [1.0, 0.0, 0.0, 0.0, 35.0, 58000.0], [0.0, 0.0, 1.0, 0.0, 38.27272727272727, 52000.0], [1.0, 0.0, 0.0, 0.0, 48.0, 79000.0], [0.0, 1.0, 0.0, 0.0, 50.0, 83000.0], [1.0, 0.0, 0.0, 0.0, 37.0, 67000.0], [0.0, 0.0, 1.0, 0.0, 44.0, 0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\618757\\Anaconda3\\envs\\mypika\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\618757\\Anaconda3\\envs\\mypika\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "                                    *** ONE HOT ENCODING ****\n",
    "A. In case of One hot encoding, For given categorical feature,new columns are being added into dataset correspoding to all unique values for that featur \n",
    "   and column contains either '0' or '1' based on the value present.\n",
    "B. Before running One Hot encoding, categorical values need to be converrted into numeric values by Label Encoder\n",
    "C. Here catagorical_features is paarameters and sparse used to make output easily readable\n",
    "D. SAVE these encoders as a file so that it can be used further on test/validation data while validation/testing a model\n",
    "\n",
    "'''\n",
    "\n",
    "onehotencoder=OneHotEncoder(categorical_features=[0],sparse=False) \n",
    "X=onehotencoder.fit_transform(X) #.tolist()\n",
    "print('After One Hot Encoder')\n",
    "print(onehot_encoder_df)\n",
    "\n",
    "pickle_out = open(\"onehot_encoded_dataprocessing.pickle\",\"wb\")\n",
    "pickle.dump(onehotencoder, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.          0.          0.77099198  0.69982954]\n",
      " [ 0.          0.          0.          1.         -1.51750803 -0.49213819]\n",
      " [ 0.          1.          0.          0.         -0.57518449 -0.09481562]\n",
      " [ 0.          0.          1.          0.         -1.92136097  0.153511  ]\n",
      " [ 0.          0.          0.          1.         -0.0367139   0.153511  ]\n",
      " [ 0.          1.          0.          0.          0.23252139  0.        ]\n",
      " [ 1.          0.          0.          0.         -0.44056685  0.00451503]\n",
      " [ 0.          0.          1.          0.          0.         -0.2934769 ]\n",
      " [ 1.          0.          0.          0.          1.30946257  1.0474868 ]\n",
      " [ 0.          1.          0.          0.          1.57869787  1.24614809]\n",
      " [ 1.          0.          0.          0.         -0.17133155  0.45150293]\n",
      " [ 0.          0.          1.          0.          0.77099198 -2.87607367]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "            *** STANDARD SCALAR or MIN-MAX SCALAR *******\n",
    "A. It is used to scale down any numeric values which are fall under high range such as Salary range is between 43 to 80K around.\n",
    "B. In this example, as Age and Salary are in high range so we are scaling down \n",
    "C. It has also to be saved as a file so that it can be used on Test data \n",
    "\n",
    "'''\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "X[:, 4:6] = sc_X.fit_transform(X[:, 4:6])\n",
    "\n",
    "pickle_out = open(\"standard_scalar_dataprocessing.pickle\",\"wb\")\n",
    "pickle.dump(sc_X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "print(X)\n",
    "\n",
    "'''\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc_X = MinMaxScaler()\n",
    "X[:, 4:6] = scaler.fit_transform(X[:, 4:6])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.          0.         -0.44056685  0.00451503]\n",
      " [ 0.          0.          1.          0.          0.77099198 -2.87607367]\n",
      " [ 0.          0.          0.          1.         -0.0367139   0.153511  ]]\n",
      "[[ 1.          0.          0.          0.         -0.17133155  0.45150293]\n",
      " [ 0.          1.          0.          0.         -0.57518449 -0.09481562]\n",
      " [ 1.          0.          0.          0.          1.30946257  1.0474868 ]\n",
      " [ 0.          0.          0.          1.         -1.51750803 -0.49213819]\n",
      " [ 0.          0.          1.          0.          0.         -0.2934769 ]\n",
      " [ 0.          1.          0.          0.          1.57869787  1.24614809]\n",
      " [ 0.          0.          1.          0.         -1.92136097  0.153511  ]\n",
      " [ 1.          0.          0.          0.          0.77099198  0.69982954]\n",
      " [ 0.          1.          0.          0.          0.23252139  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Spliting test and train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state=0)\n",
    "print(X_test)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Simple Regression to the training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "lrmodel = regressor.fit(X_train, Y_train)\n",
    "\n",
    "with open('model_pkl.pkl', 'wb') as fid:\n",
    "    pickle.dump(lrmodel, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84763177, 0.51989443, 0.53643998, 1.        , 0.14120207,\n",
       "       0.18916905, 0.85879793, 0.61592825, 0.29093652])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting test set results\n",
    "Y_pred = regressor.predict(X_train)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 34.0 56000.0]\n",
      " ['India' 24.0 61000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['India' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]\n",
      " ['India' 44.0 0.0]]\n",
      "After SimpleImputer\n",
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 34.0 56000.0]\n",
      " ['India' 24.0 61000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 57909.09090909091]\n",
      " ['France' 35.0 58000.0]\n",
      " ['India' 38.27272727272727 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]\n",
      " ['India' 44.0 0.0]]\n",
      "After Label Encoding\n",
      "[[0 44.0 72000.0]\n",
      " [3 27.0 48000.0]\n",
      " [1 34.0 56000.0]\n",
      " [2 24.0 61000.0]\n",
      " [3 38.0 61000.0]\n",
      " [1 40.0 57909.09090909091]\n",
      " [0 35.0 58000.0]\n",
      " [2 38.27272727272727 52000.0]\n",
      " [0 48.0 79000.0]\n",
      " [1 50.0 83000.0]\n",
      " [0 37.0 67000.0]\n",
      " [2 44.0 0.0]]\n",
      "After one hot Encoding\n",
      "[[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.40000000e+01 7.20000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  2.70000000e+01 4.80000000e+04]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.40000000e+01 5.60000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  2.40000000e+01 6.10000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  3.80000000e+01 6.10000000e+04]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.00000000e+01 5.79090909e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.50000000e+01 5.80000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  3.82727273e+01 5.20000000e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.80000000e+01 7.90000000e+04]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  5.00000000e+01 8.30000000e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.70000000e+01 6.70000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  4.40000000e+01 0.00000000e+00]]\n",
      "After Standard Scalar\n",
      "[[ 1.          0.          0.          0.          0.77099198  0.69982954]\n",
      " [ 0.          0.          0.          1.         -1.51750803 -0.49213819]\n",
      " [ 0.          1.          0.          0.         -0.57518449 -0.09481562]\n",
      " [ 0.          0.          1.          0.         -1.92136097  0.153511  ]\n",
      " [ 0.          0.          0.          1.         -0.0367139   0.153511  ]\n",
      " [ 0.          1.          0.          0.          0.23252139  0.        ]\n",
      " [ 1.          0.          0.          0.         -0.44056685  0.00451503]\n",
      " [ 0.          0.          1.          0.          0.         -0.2934769 ]\n",
      " [ 1.          0.          0.          0.          1.30946257  1.0474868 ]\n",
      " [ 0.          1.          0.          0.          1.57869787  1.24614809]\n",
      " [ 1.          0.          0.          0.         -0.17133155  0.45150293]\n",
      " [ 0.          0.          1.          0.          0.77099198 -2.87607367]]\n",
      "[ 0.61592825  1.          0.51989443  0.85879793  0.70158049  0.29093652\n",
      "  0.81712837  0.14120207  0.53643998  0.18916905  0.84763177 -0.76478233]\n"
     ]
    }
   ],
   "source": [
    "test_data = dataset.iloc[:,:-1].values\n",
    "print(test_data)\n",
    "\n",
    "simple_imputer_file = open(\"SimpleImputer_dataprocessing.pickle\",\"rb\")\n",
    "simple_imputer_file_pickle = pickle.load(simple_imputer_file)\n",
    "test_data[:,1:3] = simple_imputer_file_pickle.transform(test_data[:,1:3])\n",
    "print('After SimpleImputer')\n",
    "print(test_data)\n",
    "\n",
    "pickle_label_in = open(\"label_encoded_dataprocessing_1.pickle\",\"rb\")\n",
    "label_encoded_pickle = pickle.load(pickle_label_in)\n",
    "test_data[:, 0] = label_encoded_pickle.transform(test_data[:, 0])\n",
    "\n",
    "print('After Label Encoding')\n",
    "print(test_data)\n",
    "\n",
    "\n",
    "pickle_in = open(\"onehot_encoded_dataprocessing.pickle\",\"rb\")\n",
    "onehot_encoded_pickle = pickle.load(pickle_in)\n",
    "data_tranform = onehot_encoded_pickle.transform(test_data) #.tolist()\n",
    "\n",
    "print('After one hot Encoding')\n",
    "print(data_tranform)\n",
    "\n",
    "standard_scalar_in = open(\"standard_scalar_dataprocessing.pickle\",\"rb\")\n",
    "standard_scalar_in_pickle = pickle.load(standard_scalar_in)\n",
    "data_tranform[:, 4:6] = standard_scalar_in_pickle.transform(data_tranform[:, 4:6])\n",
    "\n",
    "print('After Standard Scalar')\n",
    "print(data_tranform)\n",
    "\n",
    "with open('model_pkl.pkl', 'rb') as fid:\n",
    "    sv = pickle.load(fid)\n",
    "\n",
    "print(sv.predict(data_tranform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\618757\\Anaconda3\\envs\\mypika\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\618757\\Anaconda3\\envs\\mypika\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\618757\\Anaconda3\\envs\\mypika\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06842022  3.4424802   0.         ...  1.          0.\n",
      "   0.        ]\n",
      " [-2.24909235  2.28647622  0.         ...  1.          0.\n",
      "   0.        ]\n",
      " [-2.16497396  2.28647622  0.         ...  1.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.26254556 -0.50407824  1.         ...  0.          0.\n",
      "   1.        ]\n",
      " [-0.22372049 -0.50407824  1.         ...  0.          0.\n",
      "   1.        ]\n",
      " [-0.06842022 -0.49151035  0.         ...  0.          0.\n",
      "   1.        ]]\n",
      "model score: 0.748\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A. We can create a pipeline to combine all pre-processing logic in single line and run it to transform the data. \n",
    "B. We can save this pipeline as a file which can be used further for test data pre-processing.\n",
    "C. This is a very neat and clean recommended approach.\n",
    "\n",
    "'''\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "titanic_url = ('https://raw.githubusercontent.com/amueller/'\n",
    "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
    "data = pd.read_csv(titanic_url)\n",
    "\n",
    "X = data.drop('survived', axis=1)\n",
    "y = data['survived']\n",
    "\n",
    "numerical_attribs = ['age', 'fare']\n",
    "categorical_attribs = ['embarked', 'sex', 'pclass']\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', Imputer(strategy = 'mean')),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehotencoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "data_preprocessor = ColumnTransformer(transformers = [\n",
    "    ('numeric', num_pipeline, numerical_attribs),\n",
    "    ('category', cat_pipeline, categorical_attribs)\n",
    "])\n",
    "\n",
    "\n",
    "'''\n",
    "if you want to see whether data preprocessor is working correctly or not- so you can try to run \n",
    "below method [data_preprocessor.fit_transform(X)] to see the transformed data before going for model building.\n",
    "'''\n",
    "transformed_dataset = data_preprocessor.fit_transform(X)\n",
    "print(transformed_dataset)\n",
    "\n",
    "\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', data_preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % model_pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
